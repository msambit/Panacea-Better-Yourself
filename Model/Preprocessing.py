# -*- coding: utf-8 -*-
"""Mental_Health_Cloud_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LBdXzaB6DeZAxHca0CK8zq8wou1b4zpL

# Read Data
"""

!pip install transformers
import pandas as pd
import numpy as np
import tensorflow as tf
import torch
from torch.nn import BCEWithLogitsLoss, BCELoss, CrossEntropyLoss , MultiLabelSoftMarginLoss, MultiLabelMarginLoss , MultiMarginLoss
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score
import pickle
from tqdm import tqdm, trange
from ast import literal_eval
import json
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler , RobertaForSequenceClassification

df = pd.read_csv('https://raw.githubusercontent.com/Nakul24-1/Emo-Cause/main/GoEmotions.csv')
df.drop(columns = ['id','author','subreddit','link_id',	'parent_id','created_utc','rater_id'],inplace = True)
df = df.dropna()

#df.iloc[:,2::]

"""# Decoding and tranformation"""

from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
labels = np.array(['admiration',
 'amusement',
 'anger',
 'annoyance',
 'approval',
 'caring',
 'confusion',
 'curiosity',
 'desire',
 'disappointment',
 'disapproval',
 'disgust',
 'embarrassment',
 'excitement',
 'fear',
 'gratitude',
 'grief',
 'joy',
 'love',
 'nervousness',
 'optimism',
 'pride',
 'realization',
 'relief',
 'remorse',
 'sadness',
 'surprise',
 'neutral']).reshape(-1, 1)
encoder = enc.fit(labels)

decoded_labels = encoder.inverse_transform(df.iloc[:,2::])
decoded_labels

js = '{"anger": ["anger", "annoyance", "disapproval"], \
"disgust": ["disgust"],"anxiety": ["fear", "nervousness"], \
"joy": ["joy", "amusement", "approval", "excitement", "gratitude",  "love", "optimism", "relief", "pride", "admiration", "desire", "caring"], \
"depression": ["sadness", "disappointment", "embarrassment", "grief",  "remorse"], \
"surprise": ["surprise", "realization", "confusion", "curiosity"]}'
mapping_dict = json.loads(js)

emotions = df.columns.to_list()[2::]
emotions

new_dict = {}
for key, values in mapping_dict.items():
  for value in values:
    if value in new_dict:
      new_dict[value].append(key)
    else:
      new_dict[value]=key

df2 = df.iloc[:,0:2]
df2['Label'] = decoded_labels
df2['new'] = df2['Label'].map(new_dict)

df2.head()

df2.drop(['Label','example_very_unclear'],axis = 1,inplace = True)

df2 = df2.dropna()

df2

"""## One Hot Encoding"""

enc = OneHotEncoder(handle_unknown='ignore')
Labels_array = enc.fit_transform(df2[['new']]).toarray()
Labels_array[0:5]

enc.categories_[0].tolist()

emd = dict(zip([0,1,2,3,4,5], enc.categories_[0].tolist()))

"""Sampling"""

df2[enc.categories_[0].tolist()] = Labels_array
df2

cols = df2.columns
label_cols = list(cols[2:])
num_labels = len(label_cols)
print('Label columns: ', label_cols)

df2['one_hot_labels'] = list(df2[label_cols].values)
df2.head()

print('Count of 1 per label: \n', df2[label_cols].sum(), '\n') # Label counts, may need to downsample or upsample
print('Count of 0 per label: \n', df2[label_cols].eq(0).sum())

"""## Make samples """

df2 = df2.groupby(label_cols).sample(3200, random_state=2022)
print('Count of 1 per label: \n', df2[label_cols].sum(), '\n') # Label counts, may need to downsample or upsample
print('Count of 0 per label: \n', df2[label_cols].eq(0).sum())

labels = list(df2.one_hot_labels.values)
comments = list(df2.text.values)

df2.sample(frac=1)

len(max(comments, key = len))

# Identifying indices of 'one_hot_labels' entries that only occur once - this will allow us to stratify split our training data later
label_counts = df2.one_hot_labels.astype(str).value_counts()
one_freq = label_counts[label_counts==1].keys()
one_freq_idxs = sorted(list(df2[df2.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)
print('df label indices with only one instance: ', one_freq_idxs)

from transformers import RobertaTokenizer
max_length = 128
tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=False) # tokenizer
encodings = tokenizer.batch_encode_plus(comments,max_length=max_length,truncation=True,padding=True,add_special_tokens=True,return_attention_mask=True) # tokenizer's encoding method
print('tokenizer outputs: ', encodings.keys())

len(comments)

input_ids = encodings['input_ids'] # tokenized and encoded sentences
token_type_ids = [0]*len(comments) # token type ids
attention_masks = encodings['attention_mask'] # attention masks

one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]
one_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]
one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]
one_freq_labels = [labels.pop(i) for i in one_freq_idxs]

train_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,
                                                            random_state=2022, test_size=0.20, stratify = labels)

# Add one frequency data to train data
train_inputs.extend(one_freq_input_ids)
train_labels.extend(one_freq_labels)
train_masks.extend(one_freq_attention_masks)
train_token_types.extend(one_freq_token_types)

# Convert all of our data into torch tensors, the required datatype for our model
train_inputs = torch.tensor(train_inputs)
train_labels = torch.tensor(train_labels)
train_masks = torch.tensor(train_masks)
train_token_types = torch.tensor(train_token_types)

validation_inputs = torch.tensor(validation_inputs)
validation_labels = torch.tensor(validation_labels)
validation_masks = torch.tensor(validation_masks)
validation_token_types = torch.tensor(validation_token_types)

batch_size = 16

# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, 
# with an iterator the entire dataset does not need to be loaded into memory

train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

torch.save(validation_dataloader,'validation_data_loader')
torch.save(train_dataloader,'train_data_loader')

model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)
model.cuda()

"""## Custom optimization parameters"""

# setting custom optimization parameters. You may implement a scheduler here as well.
param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
     'weight_decay_rate': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
     'weight_decay_rate': 0.0}
]

from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'
optimizer = torch.optim.AdamW(optimizer_grouped_parameters,lr=2e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-5)  # Default optimization

"""# Train The model"""

# Store our loss and accuracy for plotting
train_loss_set = []

# Number of training epochs (authors recommend between 2 and 4)
epochs = 4

# trange is a tqdm wrapper around the normal python range
for epoch in range(epochs):
  loop = tqdm(train_dataloader, leave=True)

  # Training
  
  # Set our model to training mode (as opposed to evaluation mode)
  model.train()

  # Tracking variables
  tr_loss = 0 #running loss
  nb_tr_examples, nb_tr_steps = 0, 0
  
  for batch in loop:
    #batch = {k: v.to(device) for k, v in batch.items()}
    batch = tuple(t.to(device) for t in batch)
    b_input_ids, b_input_mask, b_labels, b_token_types = batch
    optimizer.zero_grad()
    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) #Original
    #outputs = model(**batch)
    logits = outputs[0]
    loss_func = CrossEntropyLoss()
    #loss = loss_func(logits,b_labels) #convert labels to float for calculation
    loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation
    # loss_func = BCELoss() 
    # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation
    train_loss_set.append(loss.item())    

    # Backward pass
    loss.backward()
    # Update parameters and take a step using the computed gradient
    optimizer.step()


    loop.set_description(f'Epoch {epoch}')
    loop.set_postfix(loss=loss.item())
    
###############################################################################

  # Validation

  # Put model in evaluation mode to evaluate loss on the validation set
  model.eval()

  # Variables to gather full output
  logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]

  # Predict
  for i, batch in enumerate(validation_dataloader):
    
    batch = tuple(t.to(device) for t in batch)
    # Unpack the inputs from our dataloader
    b_input_ids, b_input_mask, b_labels, b_token_types = batch
    with torch.no_grad():
      # Forward pass
      outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
      b_logit_pred = outs[0]
      pred_label = torch.sigmoid(b_logit_pred)

      b_logit_pred = b_logit_pred.detach().cpu().numpy()
      pred_label = pred_label.to('cpu').numpy()
      b_labels = b_labels.to('cpu').numpy()

    tokenized_texts.append(b_input_ids)
    logit_preds.append(b_logit_pred)
    true_labels.append(b_labels)
    pred_labels.append(pred_label)

  # Flatten outputs
  pred_labels = [item for sublist in pred_labels for item in sublist]
  true_labels = [item for sublist in true_labels for item in sublist]

  # Calculate Accuracy
  threshold = 0.5
  pred_bools = [pl[np.argmax(pl)] == pl for pl in pred_labels]
  true_bools = [tl==1 for tl in true_labels]
  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100
  val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100

  print('F1 Validation Accuracy: ', val_f1_accuracy)
  print('Flat Validation Accuracy: ', val_flat_accuracy)

test_labels = list(df2.one_hot_labels.values)
test_comments = list(df2.text.values)

test_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)
test_input_ids = test_encodings['input_ids']
test_token_type_ids = [0]*len(test_encodings['input_ids'])
test_attention_masks = test_encodings['attention_mask']

test_inputs = torch.tensor(test_input_ids)
test_labels = torch.tensor(test_labels)
test_masks = torch.tensor(test_attention_masks)
test_token_types = torch.tensor(test_token_type_ids)
# Create test dataloader
test_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)
# Save test dataloader
torch.save(test_dataloader,'test_data_loader')

# Test

# Put model in evaluation mode to evaluate loss on the validation set
model.eval()

#track variables
logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]

# Predict
for i, batch in enumerate(test_dataloader):
  batch = tuple(t.to(device) for t in batch)
  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels, b_token_types = batch
  with torch.no_grad():
    # Forward pass
    outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
    b_logit_pred = outs[0]
    pred_label = torch.sigmoid(b_logit_pred)
    
    b_logit_pred = b_logit_pred.detach().cpu().numpy()
    pred_label = pred_label.to('cpu').numpy()
    b_labels = b_labels.to('cpu').numpy()

  tokenized_texts.append(b_input_ids)
  logit_preds.append(b_logit_pred)
  true_labels.append(b_labels)
  pred_labels.append(pred_label)

# Flatten outputs
tokenized_texts = [item for sublist in tokenized_texts for item in sublist]
pred_labels = [item for sublist in pred_labels for item in sublist]
true_labels = [item for sublist in true_labels for item in sublist]
# Converting flattened binary values to boolean values
true_bools = [tl==1 for tl in true_labels]

np123=np.row_stack(b_logit_pred)

tokenized_texts = [item for sublist in tokenized_texts for item in sublist]
pred_labels = [item for sublist in pred_labels for item in sublist]
true_labels = [item for sublist in true_labels for item in sublist]
# Converting flattened binary values to boolean values
true_bools = [tl==1 for tl in true_labels]

pred_bools = [pl[np.argmax(pl)] == pl  for pl in pred_labels]

#pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding
pred_bools = [pl[np.argmax(pl)] == pl  for pl in pred_labels]
true_bools = [pl>0.50 for pl in true_labels] #boolean output after thresholding CHANGE THIS 

# Print and save classification report
print('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))
print('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\n')
clf_report = classification_report(true_bools,pred_bools,target_names=label_cols)
pickle.dump(clf_report, open('classification_report.txt','wb')) #save report
print(clf_report)

from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForQuestionAnswering , AutoModelForSequenceClassification
classification = pipeline(task="text-classification", model=model, tokenizer=tokenizer,device = 0)

emotion = classification("Is it true Life is pointless and nothing matters anymore?")

emotion

model_path = 'models/roberta'
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path,id2label = emd)

## HUGGING FACE

!pip install huggingface_hub

from huggingface_hub import notebook_login

notebook_login()

model.push_to_hub("Nakul24/RoBERTa-Goemotions-6")

tokenizer.push_to_hub("Nakul24/RoBERTa-Goemotions-6")

